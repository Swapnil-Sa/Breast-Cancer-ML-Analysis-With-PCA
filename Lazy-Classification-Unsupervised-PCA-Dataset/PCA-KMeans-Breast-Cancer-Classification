# -*- coding: utf-8 -*-
"""RBL 5th Semester

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1jfJwgUOr4BKi6q1FEXSYas97ZSL53_Ib

# **Eigenvalue & Eigenvector**
"""

import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler

# Load the dataset
df = pd.read_csv('/content/WBC data.csv')

# Remove unnamed columns (caused by extra commas in header)
unnamed = [col for col in df.columns if "unnamed" in col.lower()]
if unnamed:
    df = df.drop(columns=unnamed)
    print(f"Removed unnamed columns: {unnamed}")
print("After removing unnamed columns:", df.shape)

# Remove the 'id' column if it exists
if 'id' in df.columns:
    df = df.drop(columns=['id'])
    print("Removed 'id' column.")
print("After removing 'id':", df.shape)

# Remove duplicate rows
df = df.drop_duplicates()
print("After dropping duplicates:", df.shape)

# Remove rows with any missing values
df = df.dropna()
print("After dropping rows with missing values:", df.shape)

# Select only numeric columns for PCA
numeric_df = df.select_dtypes(include=[np.number])
print("Numeric columns used for PCA:", numeric_df.columns.tolist())

# Standardize the numeric features (very important for PCA!)
scaler = StandardScaler()
X_std = scaler.fit_transform(numeric_df)  # shape: (samples, features)
print("Standardized data shape:", X_std.shape)

import numpy as np
import pandas as pd

# Use standardized data from Step 1
cov_matrix = np.cov(X_std, rowvar=False)  # columns=features

# Optional: Put in DataFrame for readability
cov_df = pd.DataFrame(cov_matrix, index=numeric_df.columns, columns=numeric_df.columns)
print("\n=== Covariance Matrix ===")
print(cov_df)

import numpy as np
import pandas as pd

# Eigen decomposition
eigvals, eigvecs = np.linalg.eigh(cov_matrix)

# Sort in descending order
sorted_idx = np.argsort(eigvals)[::-1]
sorted_eigvals = eigvals[sorted_idx]
sorted_eigvecs = eigvecs[:, sorted_idx]

# Keep top 2 PCs
top_k = 10
top2_eigvals = sorted_eigvals[:top_k]
top2_eigvecs = sorted_eigvecs[:, :top_k]

# Display
print("=== Top 10 Eigenvalues (sorted) ===")
print(top2_eigvals)

print("\n=== Eigenvectors for Top 10 Principal Components ===")
eigvecs_df = pd.DataFrame(
    top2_eigvecs,
    index=numeric_df.columns,
    columns=[f"PC{i+1}" for i in range(top_k)]
)
print(eigvecs_df)

display(eigvecs_df)

import pandas as pd

# Suppose your PCA results are stored in a DataFrame called pca_df
# Example:
# pca_df = pd.DataFrame(pca_result, columns=[f"PC{i}" for i in range(1, 11)])

# Save to CSV
eigvecs_df.to_csv("pca_results.csv", index=False)

print("âœ… PCA results saved as pca_results.csv")

import pandas as pd
import matplotlib.pyplot as plt

# Assuming sorted_eigvecs and numeric_df.columns are available
pc1_loadings = pd.Series(sorted_eigvecs[:,0], index=numeric_df.columns)
pc1_loadings.sort_values().plot(kind='barh', figsize=(8,6))
plt.title('Feature Loadings for PC1')
plt.xlabel('Loading Value')
plt.show()

import seaborn as sns

n_pcs = 10
plt.figure(figsize=(10,8))
sns.heatmap(sorted_eigvecs[:,:n_pcs], annot=True, cmap='coolwarm',
            yticklabels=numeric_df.columns, xticklabels=[f'PC{i+1}' for i in range(n_pcs)])
plt.title('Feature Loadings Heatmap (First 10 PCs)')
plt.show()

import numpy as np
import pandas as pd

# Sorted eigenvalues and eigenvectors from Step 3
num_pcs = 10  # How many PCs to keep

# Calculate explained variance
explained_variance = sorted_eigvals / np.sum(sorted_eigvals)
cumulative_variance = np.cumsum(explained_variance)

print("Explained variance (first 10 PCs):", explained_variance[:num_pcs])
print("Cumulative variance (first 10 PCs):", cumulative_variance[:num_pcs])

# Select top PCs
selected_eigenvectors = sorted_eigvecs[:, :num_pcs]

# Project standardized data onto the top PCs
X_pca = np.dot(X_std, selected_eigenvectors)
print("Transformed dataset shape (using first 10 PCs):", X_pca.shape)

import pandas as pd
import numpy as np

# Provided values (replace with your variables as needed)
explained_variance = np.array([
    0.44272026, 0.18971182, 0.09393163, 0.06602135, 0.05495768,
    0.04024522, 0.02250734, 0.01588724, 0.01389649, 0.01168978
])
cumulative_variance = np.array([
    0.44272026, 0.63243208, 0.72636371, 0.79238506, 0.84734274,
    0.88758796, 0.9100953, 0.92598254, 0.93987903, 0.95156881
])

# Format as percentage for clarity
explained_variance_percent = explained_variance * 100
cumulative_variance_percent = cumulative_variance * 100

# Create DataFrame
pc_labels = [f'PC{i+1}' for i in range(10)]
summary_df = pd.DataFrame({
    'Explained Variance (%)': explained_variance_percent.round(3),
    'Cumulative Variance (%)': cumulative_variance_percent.round(3)
}, index=pc_labels)

print("\nPCA Explained Variance Summary (First 10 Principal Components):\n")
print(summary_df)
print("\nInterpretation:")
print(f"- PC1 alone explains {explained_variance_percent[0]:.2f}% of the variance.")
print(f"- By PC2, you explain {cumulative_variance_percent[1]:.2f}% of the variance.")
print(f"- By PC10, you explain {cumulative_variance_percent[9]:.2f}% of the variance (almost all).")

import matplotlib.pyplot as plt

plt.figure(figsize=(8,6))
plt.scatter(X_pca[:,0], X_pca[:,1], alpha=0.7)
plt.xlabel('PC1')
plt.ylabel('PC2')
plt.title('Samples projected onto first 2 PCs')
plt.grid(True)
plt.show()

import pandas as pd
from sklearn.cluster import KMeans

# Step 1: Use PCA components (last 10 columns if eigvecs_df has them)
X = eigvecs_df.iloc[:, -10:]   # select 10 PCA components

# Step 2: Apply K-Means with tuned hyperparameters
kmeans = KMeans(
    n_clusters=2,          # binary clustering (benign/malignant)
    random_state=42,
    n_init=1500,             # more initializations for stability
    max_iter=500,          # allow more iterations
    tol=1e-5,              # stricter convergence tolerance
    init="k-means++"       # smarter centroid initialization
)
clusters = kmeans.fit_predict(X)

# Step 3: Add clusters back to DataFrame
eigvecs_df["Cluster"] = clusters

print(eigvecs_df.head())

display(eigvecs_df)

eigvecs_df.to_csv("kmeans_pca_results.csv", index=False,  header=True)

print("âœ… PCA results saved as pca_results.csv")

import pandas as pd
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score


X = eigvecs_df.values   # shape (30, 10)

# Apply KMeans
kmeans = KMeans(n_clusters=2, random_state=42, n_init=10)
labels = kmeans.fit_predict(X)

# Silhouette Score
score = silhouette_score(X, labels)
print(f"Silhouette Score: {score:.4f}")

""".

# **Lazy Classification**
"""

!pip install lazypredict

# ðŸ“Œ Install dependencies
!pip install scikit-learn matplotlib seaborn pandas

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import StratifiedKFold, cross_val_predict
from sklearn.metrics import confusion_matrix, classification_report
from sklearn.svm import LinearSVC
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier
import warnings
warnings.filterwarnings("ignore")

# ===============================
# 1. Load dataset
# ===============================
df = pd.read_csv("/content/kmeans_pca_results.csv")

# Features = PCs, Target = Cluster
X = df.drop("Cluster", axis=1)
y = df["Cluster"]

# ===============================
# 2. Define top 4 models
# ===============================
models = {
    "LinearSVC": LinearSVC(max_iter=5000, random_state=42),
    "LogisticRegression": LogisticRegression(max_iter=5000, random_state=42),
    "RandomForest": RandomForestClassifier(n_estimators=200, random_state=42),
    "AdaBoost": AdaBoostClassifier(n_estimators=200, random_state=42)
}

# ===============================
# 3. Cross-validation setup (use all 30 rows)
# ===============================
cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

# ===============================
# 4. Train & Evaluate
# ===============================
for name, model in models.items():
    print(f"\n===== {name} =====")
    y_pred = cross_val_predict(model, X, y, cv=cv)

    # Confusion Matrix
    cm = confusion_matrix(y, y_pred)
    plt.figure(figsize=(6,5))
    sns.heatmap(cm, annot=True, fmt="d", cmap="Blues",
                xticklabels=["Benign (B)", "Malignant (M)"],
                yticklabels=["Benign (B)", "Malignant (M)"])
    plt.xlabel("Predicted Label")
    plt.ylabel("True Label")
    plt.title(f"Confusion Matrix for {name}")
    plt.show()

    # Classification Report
    print(classification_report(y, y_pred, target_names=["Benign (B)", "Malignant (M)"]))

""".

# **Classification Threshold Tuning**
"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score
import matplotlib.pyplot as plt

data = pd.read_csv("/content/kmeans_pca_results.csv")

# Assuming last column is target
X = data.iloc[:, :-1]   # PCA features
y = data.iloc[:, -1]    # target

# ===== Encode target labels (B=0, M=1) =====
le = LabelEncoder()
y = le.fit_transform(y)

# ===== Train/test split =====
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

# (Scaling not strictly needed since PCA data is already scaled, but safe to keep)
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

from sklearn.ensemble import AdaBoostClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, roc_curve
import numpy as np

# Train AdaBoost
clf = AdaBoostClassifier(random_state=42, n_estimators=100)
clf.fit(X_train, y_train)

# Get probability predictions (values between 0 and 1)
y_proba = clf.predict_proba(X_test)[:, 1]

# ===== Predict probabilities =====
y_proba = clf.predict_proba(X_test)[:, 1]

# ===== Threshold tuning =====
thresholds = np.arange(0.0, 1.01, 0.01)
results = []

for t in thresholds:
    y_pred = (y_proba >= t).astype(int)
    acc = accuracy_score(y_test, y_pred)
    prec = precision_score(y_test, y_pred, zero_division=0)
    rec = recall_score(y_test, y_pred)
    f1 = f1_score(y_test, y_pred)
    results.append([t, acc, prec, rec, f1])

results_df = pd.DataFrame(results, columns=["Threshold", "Accuracy", "Precision", "Recall", "F1"])

# ===== Show all thresholds =====
pd.set_option("display.max_rows", None)
print(results_df)

# ===== Best threshold for each metric =====
best_accuracy = results_df.loc[results_df["Accuracy"].idxmax()]
best_precision = results_df.loc[results_df["Precision"].idxmax()]
best_recall = results_df.loc[results_df["Recall"].idxmax()]
best_f1 = results_df.loc[results_df["F1"].idxmax()]

print("\nBest Accuracy:\n", best_accuracy)
print("\nBest Precision:\n", best_precision)
print("\nBest Recall:\n", best_recall)
print("\nBest F1-score:\n", best_f1)

# ===== ROC-AUC =====
auc = roc_auc_score(y_test, y_proba)
print(f"\nROC-AUC Score: {auc:.4f}")

# ===== Plot metrics vs threshold =====
plt.figure(figsize=(10,6))
plt.plot(results_df["Threshold"], results_df["Accuracy"], label="Accuracy")
plt.plot(results_df["Threshold"], results_df["Precision"], label="Precision")
plt.plot(results_df["Threshold"], results_df["Recall"], label="Recall")
plt.plot(results_df["Threshold"], results_df["F1"], label="F1-score")

# Mark best thresholds
plt.axvline(best_accuracy["Threshold"], color="blue", linestyle="--", alpha=0.7, label=f"Best Acc {best_accuracy['Threshold']:.2f}")
plt.axvline(best_precision["Threshold"], color="orange", linestyle="--", alpha=0.7, label=f"Best Prec {best_precision['Threshold']:.2f}")
plt.axvline(best_recall["Threshold"], color="green", linestyle="--", alpha=0.7, label=f"Best Rec {best_recall['Threshold']:.2f}")
plt.axvline(best_f1["Threshold"], color="red", linestyle="--", alpha=0.7, label=f"Best F1 {best_f1['Threshold']:.2f}")

plt.xlabel("Threshold")
plt.ylabel("Score")
plt.title("Model Performance Across Thresholds (Unsupervised PCA Dataset)")
plt.legend()
plt.grid(True)
plt.show()

!pip install pandas tabulate

from tabulate import tabulate

print(tabulate(df, headers='keys', tablefmt='latex', showindex=False))

