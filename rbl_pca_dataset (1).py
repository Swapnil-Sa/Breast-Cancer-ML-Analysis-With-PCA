# -*- coding: utf-8 -*-
"""RBL PCA  Dataset

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/16B1NOqka6PHLrNXMVa8IvxK13ll2gwBX

# **FEATURE SELECTION**
"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.feature_selection import RFE
import matplotlib.pyplot as plt
import seaborn as sns

# Load the dataset
data = pd.read_csv('/content/WBC data.csv')

# Separate features and target
target = data['diagnosis'].map({'M': 1, 'B': 0})  # Map diagnosis to numerical values
features = data.drop(columns=['diagnosis'])

# Standardize the features
scaler = StandardScaler()
features_scaled = scaler.fit_transform(features)

# Feature selection using Random Forest
rf = RandomForestClassifier(random_state=42)
rf.fit(features_scaled, target)
importances = rf.feature_importances_

# Plot feature importance
plt.figure(figsize=(12, 6))
plt.title('Feature Importance from Random Forest')
feature_names = features.columns
sns.barplot(x=importances, y=feature_names)
plt.show()

# Recursive Feature Elimination (RFE) with Random Forest
rfe = RFE(estimator=rf, n_features_to_select=10)
rfe.fit(features_scaled, target)

# Selected features
selected_features = features.columns[rfe.support_]
print("\nSelected Features for PCA:", selected_features)

# Save the selected feature data for PCA
pca_data = pd.DataFrame(features_scaled[:, rfe.support_], columns=selected_features)
pca_data['diagnosis'] = target
pca_data.to_csv('reduced_features_for_pca.csv', index=False)
print('\nReduced feature dataset saved as reduced_features_for_pca.csv')

""".

# **PCA DATASET**
"""

import pandas as pd
import numpy as np
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
from sklearn.impute import SimpleImputer

data = pd.read_csv('/content/WBC data.csv')  # Adjust the path as necessary

# Display the first few rows of the dataset
print("First few rows of the dataset:")
print(data.head())

print("\nColumns in the dataset:")
print(data.columns)

# Drop unnecessary columns (e.g., 'Unnamed: 1', 'Unnamed: 32', 'Unnamed: 33')
data = data.drop(columns=['Unnamed: 1', 'Unnamed: 32', 'Unnamed: 33'], errors='ignore')

# Handle missing values (if any)
imputer = SimpleImputer(strategy='mean')
features_imputed = imputer.fit_transform(data.drop(columns=['diagnosis']))  # Adjust 'diagnosis' if necessary

# Convert to DataFrame for easier manipulation
features_df = pd.DataFrame(features_imputed, columns=data.columns[:-1])  # Exclude target variable

selected_features = ['perimeter_mean', 'area_mean', 'concavity_mean', 'concave points_mean',
       'radius_worst', 'texture_worst', 'perimeter_worst', 'area_worst',
       'concavity_worst', 'concave points_worst']  # Adjust as needed
features_selected = features_df[selected_features]

# Display the selected features
print("\nSelected features for PCA:")
print(features_selected.columns.tolist())

scaler = StandardScaler()
features_scaled = scaler.fit_transform(features_selected)

pca = PCA(n_components=2)  # We want to reduce to 2 dimensions
principal_components = pca.fit_transform(features_scaled)

# Create a DataFrame for PCA results
pca_df = pd.DataFrame(data=principal_components, columns=['PCA1', 'PCA2'])

# Check if 'diagnosis' column exists and map it to numerical values
if 'diagnosis' in data.columns:
    data['diagnosis_numeric'] = data['diagnosis'].map({'M': 1, 'B': 0})  # Malignant = 1, Benign = 0
    pca_df['Diagnosis'] = data['diagnosis_numeric']
else:
    print("Warning: 'diagnosis' column not found in the dataset.")

pca_df.to_csv('pca_results.csv', index=False)

# Display the PCA results
print("\nPCA Results:")
print(pca_df)

# Display explained variance ratio
explained_variance_ratio = pca.explained_variance_ratio_
print("\nExplained Variance Ratio:")
print(explained_variance_ratio)

display(pca_df)

""".

# **BOX PLOT**
"""

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

data = pd.read_csv('/content/pca_results.csv')
data.head()

sns.set(style="whitegrid")

# Create a box plot for PC1
plt.figure(figsize=(12, 6))
plt.subplot(1, 2, 1)  # 1 row, 2 columns, 1st subplot
# Replace 'PC1' with the actual column name from your DataFrame if it's different
# The column name was incorrect, changed from 'Diagnosis' to 'Target'
sns.boxplot(x='Diagnosis', y='PCA1', data=data, palette='Set2')  # Example: y='PCA1'
plt.title('Box Plot of Principal Component 1 (PC1)')
plt.xlabel('Diagnosis (0: Benign, 1: Malignant)')
plt.ylabel('Principal Component 1 (PC1)')

# Create a box plot for PC2
plt.subplot(1, 2, 2)  # 1 row, 2 columns, 2nd subplot
# Replace 'PC2' with the actual column name from your DataFrame if it's different
# The column name was incorrect, changed from 'Diagnosis' to 'Target'
sns.boxplot(x='Diagnosis', y='PCA2', data=data, palette='Set2')  # Example: y='PCA2'
plt.title('Box Plot of Principal Component 2 (PC2)')
plt.xlabel('Diagnosis (0: Benign, 1: Malignant)')
plt.ylabel('Principal Component 2 (PC2)')

# Show the plots
plt.tight_layout()
plt.show()

""".

# **SCATTER PLOT**
"""

sns.set(style="whitegrid")

# Create a scatter plot
plt.figure(figsize=(10, 6))
# Changed 'PC1' to 'PCA1', 'PC2' to 'PCA2', and 'Diagnosis' to 'Target'
sns.scatterplot(data=pca_df, x='PCA1', y='PCA2', hue='Diagnosis', palette='Set2', s=100, alpha=0.7)

# Add titles and labels
plt.title('Scatter Plot of PCA Results', fontsize=16)
plt.xlabel('Principal Component 1 (PC1)', fontsize=14)
plt.ylabel('Principal Component 2 (PC2)', fontsize=14)
# Updated legend labels to reflect the 'Target' column
plt.legend(title='Diagnosis', labels=['Benign (0)', 'Malignant (1)'], fontsize=12)

# Show the plot
plt.tight_layout()
plt.show()

""".

# **HEAT MAP**
"""

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

# 1. Load your dataset
data = pd.read_csv('/content/pca_results.csv')  # Replace with your actual file path

# 2. Calculate the correlation matrix
# Select only the numeric columns for correlation calculation
numeric_data = data.select_dtypes(include=['number'])
correlation_matrix = numeric_data.corr()

# 3. Create the heatmap
sns.set(style="white")
plt.figure(figsize=(10, 8))
sns.heatmap(correlation_matrix, annot=True, fmt=".2f", cmap='coolwarm', square=True, cbar_kws={"shrink": .8})
plt.title('Heatmap of Correlation Matrix', fontsize=16)
plt.xlabel("Features")
plt.ylabel("Features")
plt.tight_layout()
plt.show()

""".

# **LAZY CLASSIFICATION**
"""

!pip install lazypredict

import pandas as pd
from sklearn.model_selection import train_test_split
from lazypredict.Supervised import LazyClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns

# Load and prepare the data
df = pd.read_csv('/content/pca_results.csv')
X = df[['PCA1', 'PCA2']]
y = df['Diagnosis']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Apply LazyClassifier to get top models
clf = LazyClassifier(verbose=0, ignore_warnings=True, custom_metric=None)
models, predictions = clf.fit(X_train, X_test, y_train, y_test)

# Select the top N models (e.g., top 4)
top_n = 4  # You can adjust this to select a different number of top models
top_models = models.index[:top_n]

for model_name in top_models:
    print(f"\nTraining and Evaluating Model: {model_name}")
    # Get the actual model instead of the ColumnTransformer
    model = clf.models[model_name][1]  # Access the model, not the transformer
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)

    # Evaluate the model
    accuracy = accuracy_score(y_test, y_pred)
    conf_matrix = confusion_matrix(y_test, y_pred)
    class_report = classification_report(y_test, y_pred)

    # Display results
    print(f"\nAccuracy of {model_name}: {accuracy}")
    print("\nConfusion Matrix:")
    print(conf_matrix)
    print("\nClassification Report:")
    print(class_report)

    # Visualize confusion matrix
    plt.figure(figsize=(8, 6))
    sns.heatmap(conf_matrix, annot=True, fmt="d", cmap="Blues",
                xticklabels=['Benign (B)', 'Malignant (M)'],
                yticklabels=['Benign (B)', 'Malignant (M)'])
    plt.xlabel("Predicted Label")
    plt.ylabel("True Label")
    plt.title(f"Confusion Matrix for {model_name}")
    plt.show()

""".

# **Classification threshold tuning**
"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score
import matplotlib.pyplot as plt
from sklearn.svm import LinearSVC
from sklearn.calibration import CalibratedClassifierCV

# ===== Load PCA dataset =====
data = pd.read_csv("/content/pca_results.csv")

# Assuming last column is target
X = data.iloc[:, :-1]   # PCA features
y = data.iloc[:, -1]    # target

# ===== Encode target labels (B=0, M=1) =====
le = LabelEncoder()
y = le.fit_transform(y)

# ===== Train/test split =====
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

# (Scaling not strictly needed since PCA data is already scaled, but safe to keep)
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# ===== Train model (Random Forest example) =====
from sklearn.svm import LinearSVC # Import LinearSVC
from sklearn.calibration import CalibratedClassifierCV # Import CalibratedClassifierCV
base_clf = LinearSVC(random_state=42, max_iter=10000)  # increase max_iter to ensure convergence
clf = CalibratedClassifierCV(base_clf, cv=5)  # now gives calibrated probabilities
clf.fit(X_train, y_train)

# ===== Predict probabilities =====
# CalibratedClassifierCV provides predict_proba
y_proba = clf.predict_proba(X_test)[:, 1]


# ===== Threshold tuning =====
# Adjust thresholds to be between 0 and 1 for probabilities
thresholds = np.arange(0, 1.01, 0.01)
results = []

for t in thresholds:
    y_pred = (y_proba >= t).astype(int)
    acc = accuracy_score(y_test, y_pred)
    prec = precision_score(y_test, y_pred, zero_division=0)
    rec = recall_score(y_test, y_pred)
    f1 = f1_score(y_test, y_pred)
    results.append([t, acc, prec, rec, f1])

results_df = pd.DataFrame(results, columns=["Threshold", "Accuracy", "Precision", "Recall", "F1"])

# ===== Show all thresholds =====
pd.set_option("display.max_rows", None)
print(results_df)

# ===== Best threshold for each metric =====
best_accuracy = results_df.loc[results_df["Accuracy"].idxmax()]
best_precision = results_df.loc[results_df["Precision"].idxmax()]
best_recall = results_df.loc[results_df["Recall"].idxmax()]
best_f1 = results_df.loc[results_df["F1"].idxmax()]

print("\nBest Accuracy:\n", best_accuracy)
print("\nBest Precision:\n", best_precision)
print("\nBest Recall:\n", best_recall)
print("\nBest F1-score:\n", best_f1)

# ===== ROC-AUC =====
auc = roc_auc_score(y_test, y_proba)
print(f"\nROC-AUC Score: {auc:.4f}")

# ===== Plot metrics vs threshold =====
plt.figure(figsize=(10,6))
plt.plot(results_df["Threshold"], results_df["Accuracy"], label="Accuracy")
plt.plot(results_df["Threshold"], results_df["Precision"], label="Precision")
plt.plot(results_df["Threshold"], results_df["Recall"], label="Recall")
plt.plot(results_df["Threshold"], results_df["F1"], label="F1-score")

# Mark best thresholds
plt.axvline(best_accuracy["Threshold"], color="blue", linestyle="--", alpha=0.7, label=f"Best Acc {best_accuracy['Threshold']:.2f}")
plt.axvline(best_precision["Threshold"], color="orange", linestyle="--", alpha=0.7, label=f"Best Prec {best_precision['Threshold']:.2f}")
plt.axvline(best_recall["Threshold"], color="green", linestyle="--", alpha=0.7, label=f"Best Rec {best_recall['Threshold']:.2f}")
plt.axvline(best_f1["Threshold"], color="red", linestyle="--", alpha=0.7, label=f"Best F1 {best_f1['Threshold']:.2f}")

plt.xlabel("Threshold")
plt.ylabel("Score")
plt.title("Model Performance Across Thresholds (PCA Dataset)")
plt.legend()
plt.grid(True)
plt.show()

